{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9082166",
      "metadata": {
        "id": "e9082166"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5141df81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5141df81",
        "outputId": "b960e68b-0140-4be8-99ea-83c7a8bc6eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'd2l-zh'...\n",
            "remote: Enumerating objects: 24068, done.\u001b[K\n",
            "remote: Total 24068 (delta 0), reused 0 (delta 0), pack-reused 24068 (from 1)\u001b[K\n",
            "Receiving objects: 100% (24068/24068), 309.44 MiB | 21.22 MiB/s, done.\n",
            "Resolving deltas: 100% (16835/16835), done.\n",
            "Updating files: 100% (834/834), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/d2l-ai/d2l-zh.git\n",
        "import sys\n",
        "sys.path.append('/content/d2l-zh')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6875f27",
      "metadata": {
        "origin_pos": 0,
        "id": "e6875f27"
      },
      "source": [
        "# 用于预训练BERT的数据集\n",
        ":label:`sec_bert-dataset`\n",
        "\n",
        "为了预训练 :numref:`sec_bert`中实现的BERT模型，我们需要以理想的格式生成数据集，以便于两个预训练任务：遮蔽语言模型和下一句预测。一方面，最初的BERT模型是在两个庞大的图书语料库和英语维基百科（参见 :numref:`subsec_bert_pretraining_tasks`）的合集上预训练的，但它很难吸引这本书的大多数读者。另一方面，现成的预训练BERT模型可能不适合医学等特定领域的应用。因此，在定制的数据集上对BERT进行预训练变得越来越流行。为了方便BERT预训练的演示，我们使用了较小的语料库WikiText-2 :cite:`Merity.Xiong.Bradbury.ea.2016`。\n",
        "\n",
        "与 :numref:`sec_word2vec_data`中用于预训练word2vec的PTB数据集相比，WikiText-2（1）保留了原来的标点符号，适合于下一句预测；（2）保留了原来的大小写和数字；（3）大了一倍以上。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "342b7589",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:38.284931Z",
          "iopub.status.busy": "2023-08-18T07:00:38.284353Z",
          "iopub.status.idle": "2023-08-18T07:00:41.113963Z",
          "shell.execute_reply": "2023-08-18T07:00:41.112838Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "342b7589"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "691a2248",
      "metadata": {
        "origin_pos": 4,
        "id": "691a2248"
      },
      "source": [
        "在WikiText-2数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格。保留至少有两句话的段落。为了简单起见，我们仅使用句号作为分隔符来拆分句子。我们将更复杂的句子拆分技术的讨论留在本节末尾的练习中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "eb911790",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.118878Z",
          "iopub.status.busy": "2023-08-18T07:00:41.118515Z",
          "iopub.status.idle": "2023-08-18T07:00:41.124582Z",
          "shell.execute_reply": "2023-08-18T07:00:41.123696Z"
        },
        "origin_pos": 5,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eb911790",
        "outputId": "32f8bf64-e9b6-4ed7-b5b8-5da00bffc3a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nparagraphs是一个列表，元素是列表。类似：\\n [\\n    ['sentence1', 'sentence2', 'sentence3', ...],\\n    ['sentence1', 'sentence2', ...],\\n    ...\\n]\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['wikitext-2'] = (\n",
        "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
        "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
        "\n",
        "#@save\n",
        "def _read_wiki(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # 大写字母转换为小写字母\n",
        "    paragraphs = [line.strip().lower().split(' . ')\n",
        "                  for line in lines if len(line.split(' . ')) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs\n",
        "\"\"\"\n",
        "paragraphs是一个列表，元素是列表。类似：\n",
        " [\n",
        "    ['sentence1', 'sentence2', 'sentence3', ...],\n",
        "    ['sentence1', 'sentence2', ...],\n",
        "    ...\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f5515b",
      "metadata": {
        "origin_pos": 6,
        "id": "f2f5515b"
      },
      "source": [
        "## 为预训练任务定义辅助函数\n",
        "\n",
        "在下文中，我们首先为BERT的两个预训练任务实现辅助函数。这些辅助函数将在稍后将原始文本语料库转换为理想格式的数据集时调用，以预训练BERT。\n",
        "\n",
        "### 生成下一句预测任务的数据\n",
        "\n",
        "根据 :numref:`subsec_nsp`的描述，`_get_next_sentence`函数生成二分类任务的训练样本。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "246ca273",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.128645Z",
          "iopub.status.busy": "2023-08-18T07:00:41.128375Z",
          "iopub.status.idle": "2023-08-18T07:00:41.133471Z",
          "shell.execute_reply": "2023-08-18T07:00:41.132347Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "246ca273",
        "outputId": "e626ed30-e456-407d-f8b1-e4b60d7a8e75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nparagraphs = [\\n    [\"the cat sat on the mat\", \"it was sleepy\"],\\n    [\"dogs are loyal\", \"they love humans\"]\\n]\\n_get_next_sentence(\"the cat sat on the mat\", \"it was sleepy\", paragraphs)\\n返回的结果是：\\n(\\'the cat sat on the mat\\', \\'they love humans\\', False) 或者\\n(\\'the cat sat on the mat\\', \\'it was sleepy\\', True)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#@save\n",
        "# 有50%概率把第二个句子换成非next的句子。\n",
        "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
        "    if random.random() < 0.5:\n",
        "        is_next = True\n",
        "    else:\n",
        "        # paragraphs是三重列表的嵌套\n",
        "        next_sentence = random.choice(random.choice(paragraphs))\n",
        "        is_next = False\n",
        "    return sentence, next_sentence, is_next\n",
        "# 调用示例：\n",
        "\"\"\"\n",
        "paragraphs = [\n",
        "    [\"the cat sat on the mat\", \"it was sleepy\"],\n",
        "    [\"dogs are loyal\", \"they love humans\"]\n",
        "]\n",
        "_get_next_sentence(\"the cat sat on the mat\", \"it was sleepy\", paragraphs)\n",
        "返回的结果是：\n",
        "('the cat sat on the mat', 'they love humans', False) 或者\n",
        "('the cat sat on the mat', 'it was sleepy', True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b1d432",
      "metadata": {
        "origin_pos": 8,
        "id": "13b1d432"
      },
      "source": [
        "下面的函数通过调用`_get_next_sentence`函数从输入`paragraph`生成用于下一句预测的训练样本。这里`paragraph`是句子列表，其中每个句子都是词元列表。自变量`max_len`指定预训练期间的BERT输入序列的最大长度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a7686fde",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.137934Z",
          "iopub.status.busy": "2023-08-18T07:00:41.137439Z",
          "iopub.status.idle": "2023-08-18T07:00:41.143146Z",
          "shell.execute_reply": "2023-08-18T07:00:41.142265Z"
        },
        "origin_pos": 9,
        "tab": [
          "pytorch"
        ],
        "id": "a7686fde"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@save\n",
        "\"\"\"\n",
        "paragraph: 当前段落（句子列表）。形如 [['i', 'like', 'nlp'], ['it', 'is', 'fun'], ...]。\n",
        "paragraphs: 所有段落的集合（用于在“负样本”情形下随机抽一句作为“伪下一句”）。\n",
        "vocab: 词表对象（这里传入，但本函数内未使用；后续把 tokens 映射成索引时会用到）\n",
        "max_len: 模型允许的总序列长度上限（含特殊符号）\n",
        "这个代码就是把某一个段落的句子对都变成数据。\n",
        "\"\"\"\n",
        "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
        "    nsp_data_from_paragraph = []\n",
        "    for i in range(len(paragraph) - 1):\n",
        "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
        "            paragraph[i], paragraph[i + 1], paragraphs)\n",
        "        # 考虑1个''词元和2个''词元\n",
        "        # 如果太长了就过滤掉\n",
        "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
        "            continue\n",
        "        # tokens列表长度是tokena和tokenb的长度再加1个cls和2个sep\n",
        "        # ex：tokens = ['','i','like','nlp','','it','is','fun','']\n",
        "        # ex:segmenst = [0,0,0,0,0,1,1,1,1] 第一个cls和第一个sep都标0，第二个sep标1\n",
        "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
        "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
        "    return nsp_data_from_paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86277b80",
      "metadata": {
        "origin_pos": 10,
        "id": "86277b80"
      },
      "source": [
        "### 生成遮蔽语言模型任务的数据\n",
        ":label:`subsec_prepare_mlm_data`\n",
        "\n",
        "为了从BERT输入序列生成遮蔽语言模型的训练样本，我们定义了以下`_replace_mlm_tokens`函数。在其输入中，`tokens`是表示BERT输入序列的词元的列表，`candidate_pred_positions`是不包括特殊词元的BERT输入序列的词元索引的列表（特殊词元在遮蔽语言模型任务中不被预测），以及`num_mlm_preds`指示预测的数量（选择15%要预测的随机词元）。在 :numref:`subsec_mlm`中定义遮蔽语言模型任务之后，在每个预测位置，输入可以由特殊的“掩码”词元或随机词元替换，或者保持不变。最后，该函数返回可能替换后的输入词元、发生预测的词元索引和这些预测的标签。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3de2c8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.147428Z",
          "iopub.status.busy": "2023-08-18T07:00:41.146946Z",
          "iopub.status.idle": "2023-08-18T07:00:41.155481Z",
          "shell.execute_reply": "2023-08-18T07:00:41.154569Z"
        },
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "5e3de2c8"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "\"\"\"\n",
        "1.tokens比如['<cls>', 'i', 'like', 'nlp', '<sep>', 'it', 'is', 'fun', '<sep>']\n",
        "2.candidate_pred_positions:可被遮蔽的token的位置索引列表（一般不包括 <cls> 和 <sep>）。\n",
        "例如 [1, 2, 3, 5, 6, 7]\n",
        "3.num_mlm_preds:要预测的 token 总数（通常是总 token 数的 15%）\n",
        "4.vocab:词表对象，提供 vocab.idx_to_token（一个包含所有词的列表），用于随机替换\n",
        "\"\"\"\n",
        "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
        "                        vocab):\n",
        "    # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
        "    mlm_input_tokens = [token for token in tokens]\n",
        "    pred_positions_and_labels = []\n",
        "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
        "    random.shuffle(candidate_pred_positions)\n",
        "    for mlm_pred_position in candidate_pred_positions:\n",
        "        # 选择里面的num_mlm_preds个token进行mask\n",
        "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
        "            break\n",
        "        masked_token = None\n",
        "        # 80%的时间：将词替换为“<mask>”词元\n",
        "        if random.random() < 0.8:\n",
        "            masked_token = '<mask>'\n",
        "        else:\n",
        "            # 10%的时间：保持词不变\n",
        "            if random.random() < 0.5:\n",
        "                masked_token = tokens[mlm_pred_position]\n",
        "            # 10%的时间：用随机词替换该词\n",
        "            else:\n",
        "                masked_token = random.choice(vocab.idx_to_token)\n",
        "        # 让num_mlm_preds个需要被mask的token有80%概率被mask，10%概率是随机词，10%概率不变。\n",
        "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
        "        # pred_positions_and_labels是列表，长度num_mlm_preds是元素是tuple，（需要被mask的token的position，变化后的token的string）\n",
        "        pred_positions_and_labels.append(\n",
        "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
        "    return mlm_input_tokens, pred_positions_and_labels #第一个与原本的token是长度一致循序一致但是部分此被mask或者替换\n",
        "    # 第二个是一个长度为num_mlm_preds的列表，元素是tuple（需要被mask的token的position，原本的token的string 类似标签 真实值）\n",
        "    \"\"\"\n",
        "    ex：\n",
        "    mlm_input_tokens = ['<cls>', 'i', '<mask>', 'nlp', '<sep>', 'it', 'is', '<mask>', '<sep>']\n",
        "    pred_positions_and_labels = [(2, 'like'), (7, 'fun')]\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ce2383",
      "metadata": {
        "origin_pos": 12,
        "id": "81ce2383"
      },
      "source": [
        "通过调用前述的`_replace_mlm_tokens`函数，以下函数将BERT输入序列（`tokens`）作为输入，并返回输入词元的索引（在 :numref:`subsec_mlm`中描述的可能的词元替换之后）、发生预测的词元索引以及这些预测的标签索引。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841a4650",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.160061Z",
          "iopub.status.busy": "2023-08-18T07:00:41.159300Z",
          "iopub.status.idle": "2023-08-18T07:00:41.165820Z",
          "shell.execute_reply": "2023-08-18T07:00:41.164855Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "841a4650"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "\"\"\"\n",
        "tokens: 当前句子（或句子对）的 token 列表，比如['<cls>', 'i', 'like', 'nlp', '<sep>', 'it', 'is', 'fun', '<sep>']\n",
        "与前面的函数的参数tokens一样。\n",
        "这是这个函数是实现了根据输入tokens得到candidate_pred_positions然后调用前面的_replace_mlm_tokens\n",
        "最终返回：\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "示例输入\n",
        "tokens = ['<cls>', 'i', 'like', 'nlp', '<sep>', 'it', 'is', 'fun', '<sep>']\n",
        "示例输出\n",
        "vocab[mlm_input_tokens]=[...]长度与原本的tokens一致元素的token元素对应的idx （其中，mlm_input_tokens = ['<cls>', 'i', '<mask>', 'nlp', '<sep>', 'it', 'is', '<mask>', '<sep>']\n",
        "pred_positions = [2, 7]\n",
        "vocab[mlm_pred_labels] = [...]长度与pred_positions一致，元素是idx （其中，mlm_pred_labels = ['like', 'fun']\n",
        "\"\"\"\n",
        "def _get_mlm_data_from_tokens(tokens, vocab):\n",
        "    candidate_pred_positions = []\n",
        "    # tokens是一个字符串列表\n",
        "    for i, token in enumerate(tokens):\n",
        "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
        "        if token in ['<cls>', '<sep>']:\n",
        "            continue\n",
        "        candidate_pred_positions.append(i) #构造 需要去除cls、sep的token列表\n",
        "    # 遮蔽语言模型任务中预测15%的随机词元\n",
        "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
        "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
        "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
        "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
        "                                       key=lambda x: x[0])\n",
        "    pred_positions = [v[0] for v in pred_positions_and_labels] # 真正被mask或者替换或未变的那15%的token的位置\n",
        "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels] # 真实值，原本的token\n",
        "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396550b1",
      "metadata": {
        "origin_pos": 14,
        "id": "396550b1"
      },
      "source": [
        "## 将文本转换为预训练数据集\n",
        "\n",
        "现在我们几乎准备好为BERT预训练定制一个`Dataset`类。在此之前，我们仍然需要定义辅助函数`_pad_bert_inputs`来将特殊的“&lt;mask&gt;”词元附加到输入。它的参数`examples`包含来自两个预训练任务的辅助函数`_get_nsp_data_from_paragraph`和`_get_mlm_data_from_tokens`的输出。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6552099b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.170203Z",
          "iopub.status.busy": "2023-08-18T07:00:41.169578Z",
          "iopub.status.idle": "2023-08-18T07:00:41.180126Z",
          "shell.execute_reply": "2023-08-18T07:00:41.179219Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "6552099b"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "\"\"\"\n",
        "examples是列表，元素是元组。即(token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) 长度由什么决定？\n",
        "其中\n",
        "来自mlm的：\n",
        "token_ids 是列表，句子A+B的词元id序列（含 [CLS], [SEP]）\n",
        "pred_positions\t被mask的token索引\n",
        "mlm_pred_label_ids\t被mask位置的真实标签\n",
        "来自nsp的：\n",
        "segments列表，长度与tokens一致\t元素：（句子A=0, 句子B=1）\n",
        "is_next\tint类型，\tNSP标签（1=是下一句，0=不是）\n",
        "\n",
        "max_len 最大句子长度（统一 padding 长度）\n",
        "vocab 词表对象\n",
        "\"\"\"\n",
        "def _pad_bert_inputs(examples, max_len, vocab):\n",
        "    max_num_mlm_preds = round(max_len * 0.15) #需要被mask的词的个数\n",
        "    \"\"\"\n",
        "    all_token_ids\t(batch_size, max_len)\tpadding 后的 token 序列\n",
        "    all_segments\t(batch_size, max_len)\tpadding后的segment 序列\n",
        "    valid_lens\t(batch_size,)\t每个句子的真实长度\n",
        "    all_pred_positions\t(batch_size, max_num_mlm_preds)\tMLM mask 位置\n",
        "    all_mlm_weights\t(batch_size, max_num_mlm_preds)\tMLM 权重（0/1）\n",
        "    all_mlm_labels\t(batch_size, max_num_mlm_preds)\tMLM 标签\n",
        "    nsp_labels\t(batch_size,)\tNSP标签（0/1）\n",
        "    \"\"\"\n",
        "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
        "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
        "    nsp_labels = []\n",
        "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
        "         is_next) in examples:\n",
        "        # 把tokens_ids补成长度都是max len。从而统一所有样本的token_ids长度\n",
        "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
        "            max_len - len(token_ids)), dtype=torch.long))\n",
        "        # 其余部分补0（后续可能需要valid len来解决这个补充的0？）\n",
        "        all_segments.append(torch.tensor(segments + [0] * (\n",
        "            max_len - len(segments)), dtype=torch.long))\n",
        "        # valid_lens不包括'<pad>'的计数 ，列表 长度是(样本数)\n",
        "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
        "        # 把pred_positions补成长度是max lenth\n",
        "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
        "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
        "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
        "\n",
        "        \"\"\"\n",
        "        EX：\n",
        "        mlm_pred_label_ids = [1037, 2054, 102]\n",
        "        max_num_mlm_preds = 6\n",
        "        all_mlm_weights = [1, 1, 1, 0, 0, 0]\n",
        "        \"\"\"\n",
        "        all_mlm_weights.append(\n",
        "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
        "                max_num_mlm_preds - len(pred_positions)),\n",
        "                dtype=torch.float32))\n",
        "        \"\"\"\n",
        "        EX：\n",
        "        mlm_pred_label_ids = [1037, 2054, 102]\n",
        "        max_num_mlm_preds = 6\n",
        "        则 all_mlm_labels = [1037, 2054, 102, 0, 0, 0]\n",
        "        \"\"\"\n",
        "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
        "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
        "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
        "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
        "            all_mlm_weights, all_mlm_labels, nsp_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e8a88c",
      "metadata": {
        "origin_pos": 18,
        "id": "d4e8a88c"
      },
      "source": [
        "将用于生成两个预训练任务的训练样本的辅助函数和用于填充输入的辅助函数放在一起，我们定义以下`_WikiTextDataset`类为用于预训练BERT的WikiText-2数据集。通过实现`__getitem__ `函数，我们可以任意访问WikiText-2语料库的一对句子生成的预训练样本（遮蔽语言模型和下一句预测）样本。\n",
        "\n",
        "最初的BERT模型使用词表大小为30000的WordPiece嵌入 :cite:`Wu.Schuster.Chen.ea.2016`。WordPiece的词元化方法是对 :numref:`subsec_Byte_Pair_Encoding`中原有的字节对编码算法稍作修改。为简单起见，我们使用`d2l.tokenize`函数进行词元化。出现次数少于5次的不频繁词元将被过滤掉。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d049c9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.184551Z",
          "iopub.status.busy": "2023-08-18T07:00:41.183947Z",
          "iopub.status.idle": "2023-08-18T07:00:41.192539Z",
          "shell.execute_reply": "2023-08-18T07:00:41.191426Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "c4d049c9"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "class _WikiTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, paragraphs, max_len):\n",
        "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
        "        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n",
        "        paragraphs = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs]\n",
        "        sentences = [sentence for paragraph in paragraphs\n",
        "                     for sentence in paragraph]\n",
        "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
        "        # 获取下一句子预测任务的数据\n",
        "        examples = []\n",
        "        for paragraph in paragraphs:\n",
        "            examples.extend(_get_nsp_data_from_paragraph(\n",
        "                paragraph, paragraphs, self.vocab, max_len))\n",
        "        # 获取遮蔽语言模型任务的数据\n",
        "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
        "                      + (segments, is_next))\n",
        "                     for tokens, segments, is_next in examples]\n",
        "        # 填充输入\n",
        "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
        "         self.all_pred_positions, self.all_mlm_weights,\n",
        "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
        "            examples, max_len, self.vocab)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
        "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
        "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
        "                self.nsp_labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ede31c0",
      "metadata": {
        "origin_pos": 22,
        "id": "0ede31c0"
      },
      "source": [
        "通过使用`_read_wiki`函数和`_WikiTextDataset`类，我们定义了下面的`load_data_wiki`来下载并生成WikiText-2数据集，并从中生成预训练样本。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b484a88",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.197261Z",
          "iopub.status.busy": "2023-08-18T07:00:41.196591Z",
          "iopub.status.idle": "2023-08-18T07:00:41.202074Z",
          "shell.execute_reply": "2023-08-18T07:00:41.201154Z"
        },
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "id": "9b484a88"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "def load_data_wiki(batch_size, max_len):\n",
        "    \"\"\"加载WikiText-2数据集\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
        "    paragraphs = _read_wiki(data_dir)\n",
        "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
        "                                        shuffle=True, num_workers=num_workers)\n",
        "    return train_iter, train_set.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b59eb9",
      "metadata": {
        "origin_pos": 26,
        "id": "74b59eb9"
      },
      "source": [
        "将批量大小设置为512，将BERT输入序列的最大长度设置为64，我们打印出小批量的BERT预训练样本的形状。注意，在每个BERT输入序列中，为遮蔽语言模型任务预测$10$（$64 \\times 0.15$）个位置。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a8e103",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:41.206083Z",
          "iopub.status.busy": "2023-08-18T07:00:41.205815Z",
          "iopub.status.idle": "2023-08-18T07:00:52.152614Z",
          "shell.execute_reply": "2023-08-18T07:00:52.151321Z"
        },
        "origin_pos": 27,
        "tab": [
          "pytorch"
        ],
        "id": "f1a8e103",
        "outputId": "0a8eb4d6-f80f-4b83-b4eb-92fd57643717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
          ]
        }
      ],
      "source": [
        "batch_size, max_len = 512, 64\n",
        "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
        "\n",
        "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
        "     mlm_Y, nsp_y) in train_iter:\n",
        "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
        "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
        "          nsp_y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b78dd7",
      "metadata": {
        "origin_pos": 28,
        "id": "c8b78dd7"
      },
      "source": [
        "最后，我们来看一下词量。即使在过滤掉不频繁的词元之后，它仍然比PTB数据集的大两倍以上。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b86684",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:52.159404Z",
          "iopub.status.busy": "2023-08-18T07:00:52.158958Z",
          "iopub.status.idle": "2023-08-18T07:00:52.169643Z",
          "shell.execute_reply": "2023-08-18T07:00:52.168438Z"
        },
        "origin_pos": 29,
        "tab": [
          "pytorch"
        ],
        "id": "47b86684",
        "outputId": "fe94d516-d771-4190-dbba-1e0175c8db80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20256"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "081adbe2",
      "metadata": {
        "origin_pos": 30,
        "id": "081adbe2"
      },
      "source": [
        "## 小结\n",
        "\n",
        "* 与PTB数据集相比，WikiText-2数据集保留了原来的标点符号、大小写和数字，并且比PTB数据集大了两倍多。\n",
        "* 我们可以任意访问从WikiText-2语料库中的一对句子生成的预训练（遮蔽语言模型和下一句预测）样本。\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 为简单起见，句号用作拆分句子的唯一分隔符。尝试其他的句子拆分技术，比如Spacy和NLTK。以NLTK为例，需要先安装NLTK：`pip install nltk`。在代码中先`import nltk`。然后下载Punkt语句词元分析器：`nltk.download('punkt')`。要拆分句子，比如`sentences = 'This is great ! Why not ?'`，调用`nltk.tokenize.sent_tokenize(sentences)`将返回两个句子字符串的列表：`['This is great !', 'Why not ?']`。\n",
        "1. 如果我们不过滤出一些不常见的词元，词量会有多大？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cebcf3ae",
      "metadata": {
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "cebcf3ae"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/5738)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}